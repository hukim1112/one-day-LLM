{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hukim1112/one-day-LLM/blob/main/3_Fine_tuning_a_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5cvv2hUiAtc"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTJgnYcaiAtc"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the following line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LcAaezFgFQ7"
      },
      "source": [
        "# Pytorchë¥¼ ì‚¬ìš©í•œ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1xkweyxiAtd"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "#í† í¬ë‚˜ì´ì € ë¡œë“œ: ì„¤ì •í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ì´ í† í¬ë‚˜ì´ì €ëŠ” BERT ëª¨ë¸ì— ë§ê²Œ í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "#ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì¦ˆ: map ë©”ì„œë“œë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì˜ ê° ìƒ˜í”Œì— tokenize_functionì„ ì ìš©í•´ í† í¬ë‚˜ì´ì¦ˆëœ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤. batched=Trueë¡œ ì„¤ì •í•˜ì—¬ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qa-knSs_iAtd"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë¸ì´ ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì§€ ì•Šìœ¼ë¯€ë¡œ í…ìŠ¤íŠ¸ ì—´ì„ ì œê±°í•©ë‹ˆë‹¤:\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "\n",
        "# ëª¨ë¸ì´ ì¸ìë¥¼ 'labels'ë¡œ ëª…ëª…ëœ ìƒíƒœë¡œ ê¸°ëŒ€í•˜ë¯€ë¡œ 'label' ì—´ì˜ ì´ë¦„ì„ 'labels'ë¡œ ë³€ê²½í•©ë‹ˆë‹¤:\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# ë°ì´í„°ì…‹ì˜ í˜•ì‹ì„ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  PyTorch í…ì„œë¥¼ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤:\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "# í•™ìŠµ ë°ì´í„°ì…‹ì˜ ì—´ ì´ë¦„ì„ ì¶œë ¥í•©ë‹ˆë‹¤:\n",
        "tokenized_datasets[\"train\"].column_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhLTZwtliAte"
      },
      "outputs": [],
      "source": [
        "[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48u5SfpMiAte"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9qIwy2riAte"
      },
      "outputs": [],
      "source": [
        "for batch in train_dataloader:\n",
        "    break\n",
        "{k: v.shape for k, v in batch.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqLDo_gLjOZw"
      },
      "source": [
        "from_pretrained ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. checkpointëŠ” ì´ì „ì— ì„¤ì •í•œ ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸(ì˜ˆ: \"bert-base-uncased\")ë¥¼ ê°€ë¦¬í‚µë‹ˆë‹¤.\n",
        "num_labels=2ëŠ” ë¶„ë¥˜ ì‘ì—…ì—ì„œ ì‚¬ìš©í•  í´ë˜ìŠ¤ì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ ì½”ë“œì—ì„œëŠ” ì´ì§„ ë¶„ë¥˜ ì‘ì—…(ë‘ ê°œì˜ í´ë˜ìŠ¤)ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ num_labelsë¥¼ 2ë¡œ ì„¤ì •í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BHe92wdiAtf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tD0LvB2jf5T"
      },
      "source": [
        "outputs.lossëŠ” ëª¨ë¸ì˜ ì†ì‹¤ ê°’ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì†ì‹¤ ê°’ì€ ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ì‹¤ì œ ê°’ê³¼ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë¡œ, ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ê³¼ì •ì—ì„œ ìµœì†Œí™”í•˜ë ¤ê³  í•©ë‹ˆë‹¤.\n",
        "outputs.logitsëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’(ë¡œì§“)ì…ë‹ˆë‹¤. ë¡œì§“ì€ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ëª¨ë¸ì˜ í™•ì‹ ë„ë¥¼ ë‚˜íƒ€ë‚´ë©°, ë³´í†µ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ í†µí•´ í™•ë¥ ë¡œ ë³€í™˜ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz5dcYg3iAtf"
      },
      "outputs": [],
      "source": [
        "outputs = model(**batch)\n",
        "print(outputs.loss, outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJaiXEJbjvt3"
      },
      "source": [
        "transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n",
        "ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ í•™ìŠµë¥ ì„ 5e-5ë¡œ ì„¤ì •í•œ AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvmNo_9ciAtf"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Kg7eCSjrr4"
      },
      "source": [
        "get_schedulerë¥¼ ì„í¬íŠ¸í•˜ì—¬ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "num_epochsë¥¼ 3ìœ¼ë¡œ ì„¤ì •í•˜ê³ , ì „ì²´ í•™ìŠµ ìŠ¤í… ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "ì„ í˜• ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ìƒì„±í•˜ì—¬ ì§€ì •ëœ í•™ìŠµ ìŠ¤í… ë™ì•ˆ í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fthuX5B5iAtg"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "print(num_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEt13CD1iAtg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUgCbJ72j4eM"
      },
      "source": [
        "ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜í•œ í›„, ì—í¬í¬ ìˆ˜ë§Œí¼ ë°˜ë³µí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3suvr_hQiAtg"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss #ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ì†ì‹¤ì„ ê³„ì‚°\n",
        "        loss.backward() # ì—­ì „íŒŒë¥¼ ìˆ˜í–‰\n",
        "\n",
        "        optimizer.step() #ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸\n",
        "        lr_scheduler.step() #í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cumo6g_ulyIy"
      },
      "source": [
        "ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜í•œ í›„, metricì„ ê³„ì‚°í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7eyw9YPiAth"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZkbFVOFlchw"
      },
      "source": [
        "accelerator ì‚¬ìš©\n",
        "\n",
        "Accelerate ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ëª¨ë¸ í•™ìŠµì„ ê°€ì†í™”í•˜ê³  PyTorch ê¸°ë°˜ì˜ ë¶„ì‚° í•™ìŠµì„ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf6Ejta1iAth"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
        "\n",
        "accelerator = Accelerator()\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
        "    train_dataloader, eval_dataloader, model, optimizer\n",
        ")\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dl)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dl:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnH7F3iqf3Fb"
      },
      "source": [
        "# Transformer Trainer API ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJYc4UF5pzas"
      },
      "source": [
        "ë‹¤ìŒìœ¼ë¡œ Trainer APIë¥¼ í™œìš©í•´ [glue ë°ì´í„°ì…‹](https://huggingface.co/datasets/nyu-mll/glue)ì˜ mrpc ë°ì´í„°ì…‹ì„ í•™ìŠµí•´ë´…ì‹œë‹¤!\n",
        "\n",
        "ğŸ¤— TransformersëŠ” ëª¨ë¸ì˜ í›ˆë ¨ì„ ìµœì í™”í•œ [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) í´ë˜ìŠ¤ë¥¼ ì œê³µí•˜ì—¬, ì§ì ‘ í›ˆë ¨ ë£¨í”„ë¥¼ ì‘ì„±í•˜ì§€ ì•Šê³ ë„ ì‰½ê²Œ í›ˆë ¨ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) APIëŠ” ë¡œê¹…, ê·¸ë˜ë””ì–¸íŠ¸ ì¶•ì , í˜¼í•© ì •ë°€ë„ ë“± ë‹¤ì–‘í•œ í›ˆë ¨ ì˜µì…˜ê³¼ ê¸°ëŠ¥ì„ ì§€ì›í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GLUE (General Language Understanding Evaluation) ë²¤ì¹˜ë§ˆí¬ëŠ” ì—¬ëŸ¬ ë‹¤ë¥¸ NLP ì‘ì—…ì„ í¬í•¨í•œ ë°ì´í„°ì…‹ ëª¨ìŒìœ¼ë¡œ, ì´ ì¤‘ mrpcëŠ” ë¬¸ì¥ ìŒì´ ì„œë¡œ ì˜ë¯¸ìƒìœ¼ë¡œ ë™ì¼í•œì§€ë¥¼ íŒë‹¨í•˜ëŠ” ì‘ì—…"
      ],
      "metadata": {
        "id": "OXhSIUkirvtw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyi06o97e5rO"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLzl9DwFlhR2"
      },
      "source": [
        "### Training hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGdgw7yXlhR3"
      },
      "source": [
        "ë‹¤ìŒìœ¼ë¡œ, ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì—¬ëŸ¬ í›ˆë ¨ ì˜µì…˜ì„ í™œì„±í™”í•˜ëŠ” í”Œë˜ê·¸ë“¤ì„ í¬í•¨í•˜ëŠ” [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) í´ë˜ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ê¸°ë³¸ í›ˆë ¨ [í•˜ì´í¼íŒŒë¼ë¯¸í„°](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)ë¡œ ì‹œì‘í•  ìˆ˜ ìˆì§€ë§Œ, ìµœì ì˜ ì„¤ì •ì„ ì°¾ê¸° ìœ„í•´ ì´ë¥¼ ì‹¤í—˜í•´ ë³´ì•„ë„ ì¢‹ìŠµë‹ˆë‹¤.\n",
        "\n",
        "í›ˆë ¨ ê³¼ì •ì—ì„œ ìƒì„±ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•  ìœ„ì¹˜ë¥¼ ì§€ì •í•˜ì„¸ìš”:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eXQWp_Me5rO"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLELrEovsWN-"
      },
      "outputs": [],
      "source": [
        "training_args"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì˜ˆìƒ ë ˆì´ë¸” ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "wRzq2mnQrx4d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLRjekg0e5rO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vwt-oT0de5rO"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2L36uMhe5rP"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FkOlWK5e5rP"
      },
      "outputs": [],
      "source": [
        "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "print(predictions.predictions.shape, predictions.label_ids.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwLm0tzKe5rP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "preds = np.argmax(predictions.predictions, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdH025FIlhR3"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)ëŠ” í›ˆë ¨ ì¤‘ ìë™ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì„±ëŠ¥ì„ ê³„ì‚°í•˜ê³  ë³´ê³ í•  í•¨ìˆ˜ë¥¼ [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)ì— ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤. [ğŸ¤— Evaluate](https://huggingface.co/docs/evaluate/index) ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ê°„ë‹¨í•œ ë‹¤ì–‘í•œ í‰ê°€ í•¨ìˆ˜ë¥¼ ì œê³µí•˜ë©°, ì´ë¥¼ [evaluate.load](https://huggingface.co/docs/evaluate/main/en/package_reference/loading_methods#evaluate.load) í•¨ìˆ˜ë¡œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì´ [quicktour](https://huggingface.co/docs/evaluate/a_quick_tour)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”:"
      ],
      "metadata": {
        "id": "CgZlE2YQhXwc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1WbCVQze5rP"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "metric.compute(predictions=preds, references=predictions.label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MNQtO7pe5rP"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC_LuIsslhR3"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELD9_eOflhR3"
      },
      "source": [
        "ë‹¤ìŒìœ¼ë¡œ, ëª¨ë¸, í›ˆë ¨ ì¸ì, í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹, í‰ê°€ í•¨ìˆ˜ë¥¼ í¬í•¨í•˜ì—¬ Trainer ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFYL7tSKe5rQ"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBFea17te5rQ"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgMbYeATlYq1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}