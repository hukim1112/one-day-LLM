{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hukim1112/one-day-LLM/blob/main/6_Alpaca_finetunning_with_WandB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
      "metadata": {
        "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3"
      },
      "source": [
        "# From Llama to Alpaca: Finetunning and LLM with Weights & Biases\n",
        "\n",
        "ì´ Notebookì—ì„œëŠ” ì‚¬ì „ í›ˆë ¨ëœ LLama ëª¨ë¸ì„ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ë°ì´í„°ì…‹ì— ëŒ€í•´ ë¯¸ì„¸ ì¡°ì •(fine-tuning)í•˜ëŠ” ë°©ë²•ì„ ë°°ìš¸ ê²ƒì…ë‹ˆë‹¤. davinci-003 (GPT-3)ìœ¼ë¡œ ìƒì„±ëœ ë°ì´í„° ëŒ€ì‹  GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ ë”ìš± í–¥ìƒëœ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ëŠ” ì—…ë°ì´íŠ¸ëœ ë²„ì „ì˜ Alpaca ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ê³µì‹ ì €ì¥ì†Œ í˜ì´ì§€](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n",
        "\n",
        "ì´ Notebookì€ ìµœì†Œ 24GB ë©”ëª¨ë¦¬ë¥¼ ê°–ì¶˜ A100/A10 GPUê°€ í•„ìš”í•©ë‹ˆë‹¤. ë§¤ê°œë³€ìˆ˜ë¥¼ ì¡°ì •í•˜ì—¬ T4ì—ì„œ ì‹¤í–‰í•  ìˆ˜ë„ ìˆì§€ë§Œ ì‹¤í–‰ ì‹œê°„ì´ ë§¤ìš° ê¸¸ì–´ì§‘ë‹ˆë‹¤.\n",
        "\n",
        "ì´ Notebookì—ëŠ” ì—°ê´€ í”„ë¡œì íŠ¸ì¸ ì•ŒíŒŒì¹´ì— ëŒ€í•œ ë³´ê³ ì„œ: [wandb](wandb.me/alpaca)ê°€ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ef319f-bf26-4192-8951-8d536181ab67",
      "metadata": {
        "id": "03ef319f-bf26-4192-8951-8d536181ab67"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "!pip install git+https://github.com/huggingface/transformers@v4.31-release\n",
        "!pip install pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7804f904-5746-4530-867d-c766f4501dea",
      "metadata": {
        "id": "7804f904-5746-4530-867d-c766f4501dea"
      },
      "source": [
        "## Prepare your Instruction Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59bd7517-70d9-4dee-9f92-1a2891caf385",
      "metadata": {
        "id": "59bd7517-70d9-4dee-9f92-1a2891caf385"
      },
      "source": [
        "ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ë°ì´í„°ì…‹ì€ ì‚¬ìš©ìì˜ íŠ¹ì • ë„ë©”ì¸ê³¼ ê´€ë ¨ëœ ëª…ë ¹/ê²°ê³¼ ìŒì˜ ëª©ë¡ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ë¶„ì•¼ì˜ ì§ˆë¬¸ê³¼ ë‹µë³€, ê¸°ìˆ  ë¶„ì•¼ì˜ ë¬¸ì œì™€ í•´ê²°ì±…, ë˜ëŠ” ë‹¨ìˆœíˆ ëª…ë ¹ê³¼ ê²°ê³¼ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì˜ˆë¡œëŠ” \"jsonL íŒŒì¼ì„ ì½ê³  ì²˜ìŒ 5ì¤„ì„ ì¶œë ¥í•˜ëŠ” íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•˜ë¼\"ê°€ ìˆìœ¼ë©°, ì´ë•Œ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ìœ ì‚¬í•œ ë‚´ìš©ì„ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
        "\n",
        "\n",
        "```python\n",
        "import json\n",
        "\n",
        "fname = \"my_file.json\"\n",
        "\n",
        "# read file from fname\n",
        "with open(fname, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(data[0:5])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da04c0a5-f481-4364-880d-10c254388987",
      "metadata": {
        "id": "da04c0a5-f481-4364-880d-10c254388987"
      },
      "source": [
        "the Alpaca (GPT-4 curated instructions and outputs) ë°ì´í„°ì…‹ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
      "metadata": {
        "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0665a80-6137-4a61-a3da-93bde606df04",
      "metadata": {
        "id": "b0665a80-6137-4a61-a3da-93bde606df04"
      },
      "source": [
        "ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fce67d2-3703-4042-816c-7a13ba9eab3e",
      "metadata": {
        "id": "0fce67d2-3703-4042-816c-7a13ba9eab3e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "dataset_file = \"alpaca_gpt4_data.json\"\n",
        "\n",
        "with open(dataset_file, \"r\") as f:\n",
        "    alpaca = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9618cd92-acdd-471b-9521-d55c38af8040",
      "metadata": {
        "id": "9618cd92-acdd-471b-9521-d55c38af8040"
      },
      "outputs": [],
      "source": [
        "type(alpaca), alpaca[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fXvc-WZKWcEU",
      "metadata": {
        "id": "fXvc-WZKWcEU"
      },
      "outputs": [],
      "source": [
        "len(alpaca)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e596e369-56aa-4721-9271-6686eed8fb35",
      "metadata": {
        "id": "e596e369-56aa-4721-9271-6686eed8fb35"
      },
      "source": [
        "ë°ì´í„°ì…‹ì—ëŠ” ëª…ë ¹(instruction)ê³¼ ê²°ê³¼(output)ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì€ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë˜ë¯€ë¡œ, í•œ ê°€ì§€ ë°©ë²•ì€ ë‹¨ìˆœíˆ ë‘˜ì„ ì—°ê²°(concatenate)í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ í† ëŒ€ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ìƒì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ëŠ” ì…ë ¥ê³¼ ì¶œë ¥ ìœ„ì¹˜ë¥¼ ëª…í™•í•˜ê²Œ í‘œì‹œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ëª¨ë“  ê²ƒì„ ì²´ê³„ì ì´ê²Œ ìœ ì§€í•˜ê¸° ìœ„í•´ datasetì„ W&B (Weights & Biases)ì— ê¸°ë¡í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "wandbëŠ” \"Weights & Biases\"ì˜ ì•½ìë¡œ, ë¨¸ì‹  ëŸ¬ë‹ ì‹¤í—˜ì„ ì¶”ì í•˜ê³ , ì‹œê°í™”í•˜ë©°, ê³µìœ í•˜ê¸° ìœ„í•œ íˆ´ì…ë‹ˆë‹¤. ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f98ce99-704b-469f-b490-04abe3bfc7c7",
      "metadata": {
        "id": "4f98ce99-704b-469f-b490-04abe3bfc7c7"
      },
      "source": [
        "### Train/Eval Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dfa1958-c58d-4b40-a7d9-5e0cc6d2abf5",
      "metadata": {
        "id": "5dfa1958-c58d-4b40-a7d9-5e0cc6d2abf5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "seed = 42\n",
        "\n",
        "random.seed(seed)\n",
        "random.shuffle(alpaca)  # this could also be a parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5492465c-c1b8-4f04-a154-36ce3bcb6610",
      "metadata": {
        "id": "5492465c-c1b8-4f04-a154-36ce3bcb6610"
      },
      "outputs": [],
      "source": [
        "train_dataset = alpaca[:-1000]\n",
        "eval_dataset = alpaca[-1000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72fc9e43-55b5-4a7b-902f-b8a3b82dbf06",
      "metadata": {
        "id": "72fc9e43-55b5-4a7b-902f-b8a3b82dbf06"
      },
      "source": [
        "ë°ì´í„°ì…‹ì„ í•™ìŠµ(training) ë°ì´í„°ì…‹ê³¼ í‰ê°€(evaluation) ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„í• í•˜ê³ , ì´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥í•œ í›„, Weights & Biases(W&B) í”Œë«í¼ì— ë¡œê¹…í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iQibuAgb69iA",
      "metadata": {
        "id": "iQibuAgb69iA"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdbc0fd0-de6c-447d-abb9-3176c6ceeaf6",
      "metadata": {
        "id": "fdbc0fd0-de6c-447d-abb9-3176c6ceeaf6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.DataFrame(train_dataset)\n",
        "eval_df = pd.DataFrame(eval_dataset)\n",
        "\n",
        "train_table = wandb.Table(dataframe=train_df)\n",
        "eval_table  = wandb.Table(dataframe=eval_df)\n",
        "\n",
        "train_df.to_json(\"alpaca_gpt4_train.jsonl\", orient='records', lines=True)\n",
        "eval_df.to_json(\"alpaca_gpt4_eval.jsonl\", orient='records', lines=True)\n",
        "\n",
        "with wandb.init(project=\"alpaca_ft\", job_type=\"split_data\"):\n",
        "    at = wandb.Artifact(\n",
        "        name=\"alpaca_gpt4_splitted\",\n",
        "        type=\"dataset\",\n",
        "        description=\"A GPT4 generated Alpaca like dataset for instruction finetunning\",\n",
        "        metadata={\"url\":\"https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data\"},\n",
        "    )\n",
        "    at.add_file(\"alpaca_gpt4_train.jsonl\")\n",
        "    at.add_file(\"alpaca_gpt4_eval.jsonl\")\n",
        "    wandb.log_artifact(at)\n",
        "    wandb.log({\"train_dataset\":train_table, \"eval_dataset\":eval_table})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26cb96e0-b2f2-4a79-ba65-e1c8d6395d54",
      "metadata": {
        "id": "26cb96e0-b2f2-4a79-ba65-e1c8d6395d54"
      },
      "source": [
        "ë°ì´í„°ì…‹ì„ í…Œì´ë¸”ë¡œ ë¡œê¹…í•˜ê³  ì´ê²ƒì„ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ê²€ì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ece48bbf-ddc0-4507-a733-83c5b3c1c20d",
      "metadata": {
        "id": "ece48bbf-ddc0-4507-a733-83c5b3c1c20d"
      },
      "outputs": [],
      "source": [
        "def prompt_no_input(row):\n",
        "    return (\"Below is an instruction that describes a task. \"\n",
        "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "024aec96-40fb-4417-8e64-060a301b0f0b",
      "metadata": {
        "id": "024aec96-40fb-4417-8e64-060a301b0f0b"
      },
      "outputs": [],
      "source": [
        "row = alpaca[0]\n",
        "print(prompt_no_input(row))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6f6a55-fd66-4215-bee7-1a05ef91e037",
      "metadata": {
        "id": "8e6f6a55-fd66-4215-bee7-1a05ef91e037"
      },
      "source": [
        "ì–´ë–¤ instructionì€ input ë³€ìˆ˜ ì•ˆì— contextê°€ ë“¤ì–´ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e42d41-a95a-41de-a3cc-1461d9e10ed1",
      "metadata": {
        "id": "99e42d41-a95a-41de-a3cc-1461d9e10ed1"
      },
      "outputs": [],
      "source": [
        "row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b795343f-0356-4689-8bc6-9ac650716c8b",
      "metadata": {
        "id": "b795343f-0356-4689-8bc6-9ac650716c8b"
      },
      "outputs": [],
      "source": [
        "def prompt_input(row):\n",
        "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04d0035e-96e6-4d69-ba1f-06e0051a3db6",
      "metadata": {
        "id": "04d0035e-96e6-4d69-ba1f-06e0051a3db6"
      },
      "outputs": [],
      "source": [
        "row = alpaca[9]\n",
        "print(prompt_input(row))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f74b3e-8ca2-4c57-b37f-225164c2cb5a",
      "metadata": {
        "id": "24f74b3e-8ca2-4c57-b37f-225164c2cb5a"
      },
      "source": [
        "ì¼ë‹¨ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. ë‚˜ì¤‘ì— ì ì ˆí•œ ì–‘ì˜ íŒ¨ë”©(padding)ê³¼ í•¨ê»˜ ê²°ê³¼ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee98efa-c7ed-43a9-94bc-aad7e0da735f",
      "metadata": {
        "id": "cee98efa-c7ed-43a9-94bc-aad7e0da735f"
      },
      "source": [
        "And the refactored function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4038166-085c-4b10-a56a-2bee0bd62436",
      "metadata": {
        "id": "c4038166-085c-4b10-a56a-2bee0bd62436"
      },
      "outputs": [],
      "source": [
        "def create_alpaca_prompt(row):\n",
        "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c34cc42-c38c-4e6f-bfb6-0f32d48aef5d",
      "metadata": {
        "id": "6c34cc42-c38c-4e6f-bfb6-0f32d48aef5d"
      },
      "source": [
        "## Why are we doing all this?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ee0cbc7-7d45-45b4-8e32-f3153b0d9ce2",
      "metadata": {
        "id": "7ee0cbc7-7d45-45b4-8e32-f3153b0d9ce2"
      },
      "source": [
        "ìš°ë¦¬ê°€ ì—…ë¡œë“œí•œ artifactì—ì„œ íŒŒì¼ì„ ë‹¤ì‹œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb58a76e-c50c-4431-8584-3a4597c2a0a0",
      "metadata": {
        "id": "cb58a76e-c50c-4431-8584-3a4597c2a0a0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from wandb import Api\n",
        "\n",
        "api = Api()\n",
        "artifact = api.artifact('capecape/alpaca_ft/alpaca_gpt4_splitted:v4', type='dataset')\n",
        "dataset_dir = artifact.download()\n",
        "\n",
        "def load_jsonl(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "train_dataset = load_jsonl(f\"{dataset_dir}/alpaca_gpt4_train.jsonl\")\n",
        "eval_dataset = load_jsonl(f\"{dataset_dir}/alpaca_gpt4_eval.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c31dd88-e90e-4697-b354-b39eed0fab3c",
      "metadata": {
        "id": "8c31dd88-e90e-4697-b354-b39eed0fab3c"
      },
      "source": [
        "ì•„ì£¼ íŠ¹ì •í•œ ë°©ë²•ìœ¼ë¡œ í† í°í™”ë¥¼ í•´ì•¼ë§Œ ëª¨ë¸ì´ ê²°ê³¼(output)ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41703ce9-a22d-4245-9f6c-a424afd9ef11",
      "metadata": {
        "id": "41703ce9-a22d-4245-9f6c-a424afd9ef11"
      },
      "outputs": [],
      "source": [
        "train_prompts = [create_alpaca_prompt(row) for row in train_dataset]\n",
        "eval_prompts = [create_alpaca_prompt(row) for row in eval_dataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4c16a0-989b-4e17-bc07-e1cc95971813",
      "metadata": {
        "id": "0a4c16a0-989b-4e17-bc07-e1cc95971813"
      },
      "outputs": [],
      "source": [
        "print(train_prompts[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc69153b-eb20-4f6d-afba-5b737d36320b",
      "metadata": {
        "id": "cc69153b-eb20-4f6d-afba-5b737d36320b"
      },
      "source": [
        "ìš°ë¦¬ëŠ” targetì„ ì²˜ë¦¬í•˜ê³  ë¬¸ìì—´ ì¢…ë£Œ í† í°(EOS)ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤. LLamaì˜ ê²½ìš° ì´ëŠ”: `\"</s>\"` ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06177a5c-84ff-4be3-8d2b-6a483c8ae5e4",
      "metadata": {
        "id": "06177a5c-84ff-4be3-8d2b-6a483c8ae5e4"
      },
      "outputs": [],
      "source": [
        "def pad_eos(ds):\n",
        "    EOS_TOKEN = \"</s>\"\n",
        "    return [f\"{row['output']}{EOS_TOKEN}\" for row in ds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5b21e8-3d5b-4964-be7b-faff5038f7f3",
      "metadata": {
        "id": "dc5b21e8-3d5b-4964-be7b-faff5038f7f3"
      },
      "outputs": [],
      "source": [
        "train_outputs = pad_eos(train_dataset)\n",
        "eval_outputs = pad_eos(eval_dataset)\n",
        "train_outputs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b42190f2-20cf-4960-866b-ccb7700b5b20",
      "metadata": {
        "id": "b42190f2-20cf-4960-866b-ccb7700b5b20"
      },
      "source": [
        "ì¢‹ìŠµë‹ˆë‹¤! examplesë¼ëŠ” ë³€ìˆ˜ì— ìµœì¢… ë²„ì „ì„ ì €ì¥í•©ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdea7bc4-1ec3-451e-ab00-549aa2056800",
      "metadata": {
        "id": "cdea7bc4-1ec3-451e-ab00-549aa2056800"
      },
      "outputs": [],
      "source": [
        "train_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(train_prompts, train_outputs)]\n",
        "eval_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(eval_prompts, eval_outputs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad92d95-23d5-474c-9271-59948d5dcbb0",
      "metadata": {
        "id": "bad92d95-23d5-474c-9271-59948d5dcbb0"
      },
      "source": [
        "ì´ê²ƒì´ ëª¨ë¸ì´ ë³´ê³  ë°°ìš¸ í•„ìš”ê°€ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e923b9bb-ced2-44f9-88f3-1c7690dde802",
      "metadata": {
        "id": "e923b9bb-ced2-44f9-88f3-1c7690dde802"
      },
      "outputs": [],
      "source": [
        "print(train_dataset[0][\"example\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5270873e-53d4-492b-bdce-b4d8ed8084bd",
      "metadata": {
        "id": "5270873e-53d4-492b-bdce-b4d8ed8084bd"
      },
      "source": [
        "## Converting text to numbers: Tokenizer\n",
        "\n",
        "ìš°ë¦¬ëŠ” ë°ì´í„°ì…‹ì„ í† í°ë“¤ë¡œ ë³€í™˜í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‰½ê²Œ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "720c707b-3bce-4164-b8c1-3c3122200c39",
      "metadata": {
        "id": "720c707b-3bce-4164-b8c1-3c3122200c39"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4162aec8-f2ba-45db-9633-817b416d4e57",
      "metadata": {
        "id": "4162aec8-f2ba-45db-9633-817b416d4e57"
      },
      "outputs": [],
      "source": [
        "#model_id = 'meta-llama/Llama-2-7b-hf\n",
        "model_id = 'NousResearch/Llama-2-7b-chat-hf'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "337fedfd-e238-4e86-a96b-24dfeed11f8a",
      "metadata": {
        "id": "337fedfd-e238-4e86-a96b-24dfeed11f8a"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"My experiments are going strong!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c69466-f7e6-45b8-a167-718c80cedc0f",
      "metadata": {
        "id": "20c69466-f7e6-45b8-a167-718c80cedc0f"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"My experiments are going strong!\", padding='max_length', max_length=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb1e5e29-254a-4dbf-ac02-ea6bb2a16e77",
      "metadata": {
        "id": "cb1e5e29-254a-4dbf-ac02-ea6bb2a16e77"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"My experiments are going strong!\",\n",
        "                 padding='max_length',\n",
        "                 max_length=10,\n",
        "                 return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a89a61d-34b7-4a56-b1d8-98ebcf3384d7",
      "metadata": {
        "id": "0a89a61d-34b7-4a56-b1d8-98ebcf3384d7"
      },
      "outputs": [],
      "source": [
        "tokenizer([\"My experiments are going strong!\",\n",
        "           \"I love Llamas\"],\n",
        "          padding='max_length',\n",
        "          # padding='longest',\n",
        "          max_length=10,\n",
        "          return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58586813-7918-4578-9583-df14c5a6a6ad",
      "metadata": {
        "id": "58586813-7918-4578-9583-df14c5a6a6ad"
      },
      "source": [
        "### Packing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d316d169-4292-41aa-a42d-cd49620a881c",
      "metadata": {
        "id": "d316d169-4292-41aa-a42d-cd49620a881c"
      },
      "source": [
        "ìš°ë¦¬ëŠ” ëª‡ ê°œì˜ ì§§ì€ examplesì„ ë” ê¸´ chunkë¡œ í¬ì¥(packing)í•©ë‹ˆë‹¤. ì´ê±¸ë¡œ ìš°ë¦¬ëŠ” ì¢€ ë” íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af6df54e-f6bc-4e56-aec2-014a19fc654c",
      "metadata": {
        "id": "af6df54e-f6bc-4e56-aec2-014a19fc654c"
      },
      "source": [
        "ì—¬ê¸°ì„œì˜ ì£¼ìš” ì•„ì´ë””ì–´ëŠ” ì§€ì‹œì‚¬í•­/ì¶œë ¥ ìƒ˜í”Œì´ ì§§ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‹ˆ EOS í† í°ìœ¼ë¡œ êµ¬ë¶„ì§€ì–´ ì—¬ëŸ¬ ê°œë¥¼ ì—°ê²°í•©ì‹œë‹¤. ë°ì´í„°ì…‹ì„ ì‚¬ì „ì— í† í°í™”í•˜ê³  ì‚¬ì „ì— íŒ¨í‚¹í•¨ìœ¼ë¡œì¨ ëª¨ë“  ê²ƒì„ ë” ë¹ ë¥´ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ë§Œì•½ max_seq_len = 1024ë¡œ ì„¤ì •í•œë‹¤ë©´, íŒ¨í‚¹í•˜ëŠ” ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë³´ì¼ ê²ƒì…ë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a537da60-db69-42a7-8c66-0ea3756c2847",
      "metadata": {
        "id": "a537da60-db69-42a7-8c66-0ea3756c2847"
      },
      "outputs": [],
      "source": [
        "max_sequence_len = 1024\n",
        "\n",
        "def pack(dataset, max_seq_len=max_sequence_len):\n",
        "    tkds_ids = tokenizer([s[\"example\"] for s in dataset])[\"input_ids\"]\n",
        "\n",
        "    all_token_ids = []\n",
        "    for tokenized_input in tkds_ids:\n",
        "        all_token_ids.extend(tokenized_input)# + [tokenizer.eos_token_id])\n",
        "\n",
        "    print(f\"Total number of tokens: {len(all_token_ids)}\")\n",
        "    packed_ds = []\n",
        "    for i in range(0, len(all_token_ids), max_seq_len+1):\n",
        "        input_ids = all_token_ids[i : i + max_seq_len+1]\n",
        "        if len(input_ids) == (max_seq_len+1):\n",
        "            packed_ds.append({\"input_ids\": input_ids[:-1], \"labels\": input_ids[1:]})  # this shift is not needed if using the model.loss\n",
        "    return packed_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c78b4e9-2c7f-4aa1-b738-be04bc55b06b",
      "metadata": {
        "id": "9c78b4e9-2c7f-4aa1-b738-be04bc55b06b"
      },
      "outputs": [],
      "source": [
        "train_ds_packed = pack(train_dataset)\n",
        "eval_ds_packed = pack(eval_dataset)\n",
        "len(train_ds_packed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pbPIYAop0xRo",
      "metadata": {
        "id": "pbPIYAop0xRo"
      },
      "outputs": [],
      "source": [
        "eval_ds_packed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df389230-911c-447c-a177-a18c22837020",
      "metadata": {
        "id": "df389230-911c-447c-a177-a18c22837020"
      },
      "source": [
        "ì´ë ‡ê²Œ í•˜ë©´, ê¸¸ì´ê°€ 1024ì¸ 11,000ê°œ ì´ìƒì˜ ì‹œí€€ìŠ¤ë¥¼ ì–»ê²Œ ë©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d43713b3-9f65-42a6-8338-ab0601e5f476",
      "metadata": {
        "id": "d43713b3-9f65-42a6-8338-ab0601e5f476"
      },
      "source": [
        "### DataLoader\n",
        "ì¼ë°˜ì ì¸ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ë¡œ í›ˆë ¨ì„ í•˜ê³  ì´ íŒ¨í‚¹ëœ ë°ì´í„°ì…‹ì—ì„œ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•  ê²ƒì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f342873a-29a3-4ed1-8b59-9e7f7f2a4c1d",
      "metadata": {
        "id": "f342873a-29a3-4ed1-8b59-9e7f7f2a4c1d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "batch_size = 16  # I have an A100 GPU with 40GB of RAM ğŸ˜\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_ds_packed,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=default_data_collator, # we don't need any special collator ğŸ˜\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_ds_packed,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=default_data_collator,\n",
        "    shuffle=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be2ccaf4-dfa1-4daa-9a6f-244c8cda7818",
      "metadata": {
        "id": "be2ccaf4-dfa1-4daa-9a6f-244c8cda7818"
      },
      "source": [
        "batchê°€ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ë°ì´í„°ë¡œë”ë¡œë¶€í„° ë‹¤ìŒê³¼ ê°™ì´ ìƒ˜í”Œë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffced7ec-bd1b-42b0-be64-04f3fae5df84",
      "metadata": {
        "id": "ffced7ec-bd1b-42b0-be64-04f3fae5df84"
      },
      "outputs": [],
      "source": [
        "b = next(iter(train_dataloader))\n",
        "b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33d15d30-58d2-465d-a9f4-be0eef2ea06b",
      "metadata": {
        "id": "33d15d30-58d2-465d-a9f4-be0eef2ea06b"
      },
      "source": [
        "We can alos decode the batch just to be super sure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28e18ad2-c141-4902-a5a4-24a1e743be35",
      "metadata": {
        "id": "28e18ad2-c141-4902-a5a4-24a1e743be35"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(b[\"input_ids\"][0])[:250]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0a46e93-7ec2-4365-8e50-be7bef873436",
      "metadata": {
        "id": "f0a46e93-7ec2-4365-8e50-be7bef873436"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(b[\"labels\"][0])[:250]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb99a7dc-0654-4985-ac3f-60ecdc6f6558",
      "metadata": {
        "id": "fb99a7dc-0654-4985-ac3f-60ecdc6f6558"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9757e458-14fd-4dd3-861f-efad04dce787",
      "metadata": {
        "id": "9757e458-14fd-4dd3-861f-efad04dce787"
      },
      "source": [
        "ë‹¤ìŒê³¼ ê°™ì´ ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ê´€ë¦¬í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6925a62e-d85e-4c86-8867-bee3a180fc08",
      "metadata": {
        "id": "6925a62e-d85e-4c86-8867-bee3a180fc08"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "gradient_accumulation_steps = 2\n",
        "\n",
        "config = SimpleNamespace(\n",
        "    model_id=model_id,\n",
        "    dataset_name=\"alpaca-gpt4\",\n",
        "    precision=\"bf16\",  # faster and better than fp16, requires new GPUs\n",
        "    n_freeze=24,  # How many layers we don't train, LLama 7B has 32.\n",
        "    lr=2e-4,\n",
        "    n_eval_samples=10, # How many samples to generate on validation\n",
        "    max_seq_len=max_sequence_len, # Lenght of the sequences to pack\n",
        "    epochs=3,  # we do 3 pasess over the dataset.\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,  # evey how many iterations we update the gradients, simulates larger batch sizes\n",
        "    batch_size=batch_size,  # what my GPU can handle, depends on how many layers are we training\n",
        "    log_model=False,  # upload the model to W&B?\n",
        "    gradient_checkpointing = True,  # saves even more memory\n",
        "    freeze_embed = True,  # why train this? let's keep them frozen â„ï¸\n",
        "    seed=seed,\n",
        ")\n",
        "\n",
        "config.total_train_steps = config.epochs * len(train_dataloader) // config.gradient_accumulation_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a7166a6-8e88-4f0d-bc0e-70aac292c645",
      "metadata": {
        "id": "0a7166a6-8e88-4f0d-bc0e-70aac292c645"
      },
      "outputs": [],
      "source": [
        "print(f\"We will train for {config.total_train_steps} steps and evaluate every epoch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bcd0229-5e4c-4bb5-827b-309bdbb351df",
      "metadata": {
        "id": "7bcd0229-5e4c-4bb5-827b-309bdbb351df"
      },
      "source": [
        "pretrained modelì„ ê°€ì ¸ì˜µë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51c10f7f-2551-4aa4-aef2-29c888b57a12",
      "metadata": {
        "id": "51c10f7f-2551-4aa4-aef2-29c888b57a12"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_id,\n",
        "    device_map=0,\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    use_cache=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
      "metadata": {
        "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def param_count(m):\n",
        "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
        "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
        "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
        "    return params, trainable_params\n",
        "\n",
        "params, trainable_params = param_count(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f79dbbb0-6dac-4f78-a862-8201088c9d57",
      "metadata": {
        "id": "f79dbbb0-6dac-4f78-a862-8201088c9d57"
      },
      "source": [
        "ì „ì²´ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ ê°•ë ¥í•œ ì—°ì‚°ë ¥ê³¼ ë©”ëª¨ë¦¬ë¥¼ í•„ìš”ë¡œí•˜ê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” 8ê°œì˜ layerë¥¼ íŠœë‹í•  ê²ƒ ì…ë‹ˆë‹¤. LLamaëŠ” ì´ 32ê°œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
      "metadata": {
        "id": "4c4de41e-5c10-478b-9524-f4a3119d277c"
      },
      "outputs": [],
      "source": [
        "# freeze layers (disable gradients)\n",
        "for param in model.parameters(): param.requires_grad = False\n",
        "for param in model.lm_head.parameters(): param.requires_grad = True\n",
        "for param in model.model.layers[config.n_freeze:].parameters(): param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
      "metadata": {
        "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb"
      },
      "outputs": [],
      "source": [
        "# Just freeze embeddings for small memory decrease\n",
        "if config.freeze_embed:\n",
        "    model.model.embed_tokens.weight.requires_grad_(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "284599f6-ba88-4172-a311-8e618f716b30",
      "metadata": {
        "id": "284599f6-ba88-4172-a311-8e618f716b30"
      },
      "source": [
        "ë˜í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ì„ ì‚¬ìš©í•˜ì—¬ ë” ë§ì´ ì €ì¥í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤(ì´ê²ƒì€ í›ˆë ¨ì„ ëŠë¦¬ê²Œ ë§Œë“¤ì§€ë§Œ, ì–¼ë§ˆë‚˜ ëŠë ¤ì§ˆì§€ëŠ” ì—¬ëŸ¬ë¶„ì˜ íŠ¹ì • ì„¤ì •ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤). ëŒ€ìš©ëŸ‰ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë§ì¶”ëŠ” ë°©ë²•ì— ëŒ€í•´ í—ˆê¹…í˜ì´ìŠ¤ ì›¹ì‚¬ì´íŠ¸ì— [ì¢‹ì€ ì•„í‹°í´](https://huggingface.co/docs/transformers/v4.18.0/en/performance)ì´ ìˆìœ¼ë‹ˆ í™•ì¸í•´ ë³´ì‹œê¸¸ ê¶Œì¥í•©ë‹ˆë‹¤!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "750ce64e-0088-4ca8-9bc9-60037e7110d3",
      "metadata": {
        "id": "750ce64e-0088-4ca8-9bc9-60037e7110d3"
      },
      "outputs": [],
      "source": [
        "# save more memory\n",
        "if config.gradient_checkpointing:\n",
        "    model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6232ce7a-b847-45c8-8ff7-e36249e7a060",
      "metadata": {
        "id": "6232ce7a-b847-45c8-8ff7-e36249e7a060"
      },
      "outputs": [],
      "source": [
        "params, trainable_params = param_count(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc92663c-95a4-4ecc-a9af-7a68d9648271",
      "metadata": {
        "id": "dc92663c-95a4-4ecc-a9af-7a68d9648271"
      },
      "source": [
        "### Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5374c44d-517b-42a0-ade6-297c0a5d18b3",
      "metadata": {
        "id": "5374c44d-517b-42a0-ade6-297c0a5d18b3"
      },
      "outputs": [],
      "source": [
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9,0.99), eps=1e-5)\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optim,\n",
        "    num_training_steps=config.total_train_steps,\n",
        "    num_warmup_steps=config.total_train_steps // 10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5efdd402-c851-47a5-9134-5b47b7d118e7",
      "metadata": {
        "id": "5efdd402-c851-47a5-9134-5b47b7d118e7"
      },
      "outputs": [],
      "source": [
        "def loss_fn(x, y):\n",
        "    \"A Flat CrossEntropy\"\n",
        "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24440392-9837-4cbb-873a-372f9f5aca20",
      "metadata": {
        "id": "24440392-9837-4cbb-873a-372f9f5aca20"
      },
      "source": [
        "## Testing during training\n",
        "\n",
        "ê±°ì˜ ë‹¤ ì™”ìŠµë‹ˆë‹¤, ì´ì œ ëª¨ë¸ì—ì„œ ìƒ˜í”Œë§í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ê°€ë” ëª¨ë¸ì´ ì¶œë ¥í•˜ëŠ” ê²ƒì„ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•´ ë´…ì‹œë‹¤! ê°„ë‹¨í•˜ê²Œ ëª¨ë¸.generate ë©”ì†Œë“œë¥¼ ê°ì‹¸ ë³´ê² ìŠµë‹ˆë‹¤. GenerationConfigì—ì„œ ê¸°ë³¸ ìƒ˜í”Œë§ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì ¸ì™€ í•´ë‹¹ ëª¨ë¸ IDë¥¼ ì „ë‹¬í•˜ë©´ ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a92fe7-3f9e-43e6-80b0-adbbd8b480ee",
      "metadata": {
        "id": "e0a92fe7-3f9e-43e6-80b0-adbbd8b480ee"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
        "test_config = SimpleNamespace(\n",
        "    max_new_tokens=256,\n",
        "    gen_config=gen_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ec41718-52c6-4335-a534-2790d03ba069",
      "metadata": {
        "id": "9ec41718-52c6-4335-a534-2790d03ba069"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, max_new_tokens=test_config.max_new_tokens, gen_config=gen_config):\n",
        "    tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(tokenized_prompt,\n",
        "                            max_new_tokens=max_new_tokens,\n",
        "                            generation_config=gen_config)\n",
        "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44601edc-db5b-40ea-bb74-9591ea4e7e50",
      "metadata": {
        "id": "44601edc-db5b-40ea-bb74-9591ea4e7e50"
      },
      "source": [
        "LoL ğŸ¤·"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4e39c49-ccb1-4fe6-aa30-988ea5583b99",
      "metadata": {
        "id": "b4e39c49-ccb1-4fe6-aa30-988ea5583b99"
      },
      "outputs": [],
      "source": [
        "prompt = eval_dataset[14][\"prompt\"]\n",
        "print(prompt + generate(prompt, 128))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "567920c0-005d-419d-98ab-4d797b243300",
      "metadata": {
        "id": "567920c0-005d-419d-98ab-4d797b243300"
      },
      "source": [
        "ìš°ë¦¬ëŠ” ê·¸ ê²°ê³¼ë¥¼ n ë‹¨ê³„ë§ˆë‹¤ í”„ë¡œì íŠ¸ì— í…Œì´ë¸”ë¡œ ê¸°ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac25c48-cb18-4560-b05c-1748e7d1adf5",
      "metadata": {
        "id": "bac25c48-cb18-4560-b05c-1748e7d1adf5"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def prompt_table(examples, log=False, table_name=\"predictions\"):\n",
        "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"output\", \"max_new_tokens\", \"temperature\", \"top_p\"])\n",
        "    for example in tqdm(examples, leave=False):\n",
        "        prompt, gpt4_output = example[\"prompt\"], example[\"output\"]\n",
        "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
        "        table.add_data(prompt, out, prompt+out, gpt4_output, test_config.max_new_tokens, test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
        "    if log:\n",
        "        wandb.log({table_name:table})\n",
        "    return table\n",
        "\n",
        "def to_gpu(tensor_dict):\n",
        "    return {k: v.to('cuda') for k, v in tensor_dict.items()}\n",
        "\n",
        "class Accuracy:\n",
        "    \"A simple Accuracy function compatible with HF models\"\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.tp = 0.\n",
        "    def update(self, logits, labels):\n",
        "        logits, labels = logits.argmax(dim=-1).view(-1).cpu(), labels.view(-1).cpu()\n",
        "        tp = (logits == labels).sum()\n",
        "        self.count += len(logits)\n",
        "        self.tp += tp\n",
        "        return tp / len(logits)\n",
        "    def compute(self):\n",
        "        return self.tp / self.count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a52fbc09-5d65-4265-abce-adda01d85584",
      "metadata": {
        "id": "a52fbc09-5d65-4265-abce-adda01d85584"
      },
      "source": [
        "ì›í•˜ì‹ ë‹¤ë©´ ê²€ì¦ì„ ë¹ ë¥´ê²Œ ì¶”ê°€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œ í…Œì´ë¸”ì„ ìƒì„±í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6891b2c0-f22e-4647-9ac2-e07a994f3e96",
      "metadata": {
        "id": "6891b2c0-f22e-4647-9ac2-e07a994f3e96"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate():\n",
        "    model.eval();\n",
        "    eval_acc = Accuracy()\n",
        "    loss, total_steps = 0., 0\n",
        "    for step, batch in enumerate(pbar:=tqdm(eval_dataloader, leave=False)):\n",
        "        pbar.set_description(f\"doing validation\")\n",
        "        batch = to_gpu(batch)\n",
        "        total_steps += 1\n",
        "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "            out = model(**batch)\n",
        "            loss += loss_fn(out.logits, batch[\"labels\"])  # you could use out.loss and not shift the dataset\n",
        "        eval_acc.update(out.logits, batch[\"labels\"])\n",
        "    # we log results at the end\n",
        "    wandb.log({\"eval/loss\": loss.item() / total_steps,\n",
        "               \"eval/accuracy\": eval_acc.compute()})\n",
        "    prompt_table(eval_dataset[:config.n_eval_samples], log=True)\n",
        "    model.train();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37364791-987e-4e81-9621-3094ed5bf86d",
      "metadata": {
        "id": "37364791-987e-4e81-9621-3094ed5bf86d"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "def save_model(model, model_name, models_folder=\"models\", log=False):\n",
        "    \"\"\"Save the model to wandb as an artifact\n",
        "    Args:\n",
        "        model (nn.Module): Model to save.\n",
        "        model_name (str): Name of the model.\n",
        "        models_folder (str, optional): Folder to save the model. Defaults to \"models\".\n",
        "    \"\"\"\n",
        "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
        "    file_name = Path(f\"{models_folder}/{model_name}\")\n",
        "    file_name.parent.mkdir(parents=True, exist_ok=True)\n",
        "    model.save_pretrained(file_name, safe_serialization=True)\n",
        "    # save tokenizer for easy inference\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model.name_or_path)\n",
        "    tokenizer.save_pretrained(model_name)\n",
        "    if log:\n",
        "        at = wandb.Artifact(model_name, type=\"model\")\n",
        "        at.add_dir(file_name)\n",
        "        wandb.log_artifact(at)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93834c0b-15e1-4e43-8535-dbb9f36af29d",
      "metadata": {
        "id": "93834c0b-15e1-4e43-8535-dbb9f36af29d"
      },
      "source": [
        "ëª¨ë¸ í‰ê°€ì™€ ëª¨ë¸ ì¶œë ¥ì„ tableì— ê¸°ë¡í•˜ëŠ” ë£¨í”„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e90177c-0c5a-48e7-9a39-2b9e67794814",
      "metadata": {
        "id": "6e90177c-0c5a-48e7-9a39-2b9e67794814"
      },
      "source": [
        "## The actual Loop\n",
        "- ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë° ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ë§\n",
        "- ìƒ˜í”Œë§ ë° ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì´ê²ƒì€ ë§¤ìš° ë¹ ë¥´ê²Œ í›ˆë ¨ë˜ë¯€ë¡œ ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤)\n",
        "- ìš°ë¦¬ëŠ” í† í° ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤, ì†ì‹¤ë³´ë‹¤ ë” ë‚˜ì€ ì§€í‘œì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe75078a-5d35-46bc-8f33-0e3adf52d072",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fe75078a-5d35-46bc-8f33-0e3adf52d072"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
        "           tags=[\"baseline\",\"7b\"],\n",
        "           job_type=\"train\",\n",
        "           config=config) # the Hyperparameters I want to keep track of\n",
        "\n",
        "# Training\n",
        "acc = Accuracy()\n",
        "model.train()\n",
        "train_step = 0\n",
        "for epoch in tqdm(range(config.epochs)):\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        batch = to_gpu(batch)\n",
        "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "            out = model(**batch)\n",
        "            loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps  # you could use out.loss and not shift the dataset\n",
        "            loss.backward()\n",
        "        if step%config.gradient_accumulation_steps == 0:\n",
        "            # we can log the metrics to W&B\n",
        "            wandb.log({\"train/loss\": loss.item() * config.gradient_accumulation_steps,\n",
        "                       \"train/accuracy\": acc.update(out.logits, batch[\"labels\"]),\n",
        "                       \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
        "                       \"train/global_step\": train_step})\n",
        "            optim.step()\n",
        "            scheduler.step()\n",
        "            optim.zero_grad(set_to_none=True)\n",
        "            train_step += 1\n",
        "    validate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4099bc28-e695-46a6-994c-b612f7811937",
      "metadata": {
        "id": "4099bc28-e695-46a6-994c-b612f7811937"
      },
      "outputs": [],
      "source": [
        "# we save the model checkpoint at the end\n",
        "#config.do_sample = True  # ìƒ˜í”Œë§ì„ í™œì„±í™”í•©ë‹ˆë‹¤.\n",
        "\n",
        "# del config.temperature  # temperature ì„¤ì •ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
        "# del config.top_p  # top_p ì„¤ì •ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
        "save_model(model, model_name=config.model_id.replace(\"/\", \"_\"), models_folder=\"models/\", log=config.log_model)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cec95b95-3e81-4a40-8eea-34048c992ba9",
      "metadata": {
        "id": "cec95b95-3e81-4a40-8eea-34048c992ba9"
      },
      "source": [
        "A100ì—ì„œ ì•½ 70ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a936e007-47fa-400f-873c-5e038bd685c5",
      "metadata": {
        "id": "a936e007-47fa-400f-873c-5e038bd685c5"
      },
      "source": [
        "## Full Eval Dataset evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "399fafb9-b41f-401b-a1c4-37e1ede2e639",
      "metadata": {
        "id": "399fafb9-b41f-401b-a1c4-37e1ede2e639"
      },
      "source": [
        "í‰ê°€ ë°ì´í„°ì…‹(eval_dataset)ì—ì„œ ëª¨ë¸ ì˜ˆì¸¡ì„ ë¡œê·¸í•˜ëŠ” í…Œì´ë¸”ì„ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤ (ì²˜ìŒ 250ê°œ ìƒ˜í”Œì— ëŒ€í•´ì„œ)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b89717a-ac70-43e8-9b7c-edd8d2c54cdc",
      "metadata": {
        "id": "3b89717a-ac70-43e8-9b7c-edd8d2c54cdc"
      },
      "outputs": [],
      "source": [
        "with wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
        "           job_type=\"eval\",\n",
        "           config=config): # the Hyperparameters I want to keep track of\n",
        "    model.eval();\n",
        "    prompt_table(eval_dataset[:250], log=True, table_name=\"eval_predictions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jCh-S5b1dg00",
      "metadata": {
        "id": "jCh-S5b1dg00"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}